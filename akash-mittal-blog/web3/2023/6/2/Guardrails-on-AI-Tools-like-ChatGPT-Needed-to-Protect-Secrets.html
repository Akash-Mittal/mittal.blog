<!doctype html>
<html>
 <head>
  <title>Guardrails on AI Tools like ChatGPT Needed to Protect Secrets</title>
  <meta charset="UTF-8">
  <meta name="description" content="CISOs provide insights on the guardrails needed to protect secrets when using AI tools like ChatGPT.">
  <meta name="keywords" content="AI tools, ChatGPT, CISOs, secrets, guardrails">
  <meta name="author" content="AI Writer">
  <link rel="stylesheet" type="text/css" href="style.css">
  <link rel="stylesheet" type="text/css" href="../../../../styles.css">
 </head>
 <body>
  <header>
   <h1>Guardrails on AI Tools like ChatGPT Needed to Protect Secrets</h1> <img src="images/Guardrails-on-AI-Tools-like-ChatGPT-Needed-to-Protect-Secrets.jpeg" alt="+Guardrails-on-AI-Tools-like-ChatGPT-Needed-to-Protect-Secrets+">
  </header>
  <main>
   <article>
    <p>A genius data analyst named Jane had a problem with her workload. She had stacks of data to analyze but could not spare time for them all. Her boss suggested using a new AI tool called ChatGPT that uses machine learning to predict insights from an input of data. Jane quickly gave it a try and was astonished by how it effortlessly produced insights that took her hours to generate manually. However, one day, Jane inputted sensitive data that was not supposed to leave the company's premises. Luckily, she realized it and quickly deleted the input from the AI tool. She then contacted the IT department to ensure that ChatGPT has no data retention policy even after it has processed the user's data.</p>
    <p>Stories like Jane's are the reasons why organizations need guardrails on AI tools to prevent accidental or intentional exposure of sensitive data. Chief Information Security Officers (CISOs) share insights on how to put guardrails on AI tools like ChatGPT.</p>
    <h2>Quantifiable examples of how AI tools can expose sensitive data</h2>
    <ul>
     <li>An AI tool can predict the identities of anonymous individuals from publicly available data (e.g., voter rolls, social media profiles, etc.)</li>
     <li>An AI tool can infer sensitive information from metadata (e.g., geolocation data from photos)</li>
     <li>An AI tool can erroneously predict individuals' health conditions based on their social media posts or search queries</li>
    </ul>
    <h2>Guardrails needed to protect secrets when using AI tools like ChatGPT</h2>
    <p><strong>Guardrail 1 - Data classification and segregation:</strong> Organizations should classify their data into high, medium, and low sensitivity levels. AI tools like ChatGPT should only have access to the data they need for processing. All sensitive data should be segregated from the unclassified data to prevent unauthorized access.</p>
    <p><strong>Guardrail 2 - Consent and accountability:</strong> Users should be aware of the AI tool's data processing purpose, who can access it, and how long it will be stored. The AI tool provider should be held accountable for any misuse that occurs due to the AI outputs. Regulations like the General Data Protection Regulation (GDPR) have provisions for data protection by design and by default.</p>
    <p><strong>Guardrail 3 - Human supervision and auditability:</strong> Human experts should constantly monitor the AI's output to ensure that it is unbiased and accurate. The AI's decision-making process should be explained in understandable terms, and any incorrect output should be rectified immediately. Organizations should have audit trails to track who accessed the processed data and when.</p>
    <h2>Conclusion</h2>
    <ol>
     <li>Classification and segregation of data limit AI's access to sensitive data.</li>
     <li>Consent, accountability, and human supervision ensure that the AI tool's purpose aligns with the users' expectations.</li>
     <li>Audit trails enhance transparency and the ability to correct data processing errors.</li>
    </ol>
    <p>Organizations should consider deploying these guardrails when using AI tools to process sensitive data. AI tool developers should also adhere to these guardrails when developing their tools to build trust in their brand. We must ensure that AI tools like ChatGPT work for us, not against us.</p>
   </article>
  </main>
  <footer>
   <p>Â© 2021 AI Writer. All rights reserved.</p>
   <address>Contact: support@aiwriter.com</address>
  </footer>
 <section id=social>
<h2>Curated by Team Akash.Mittal.Blog  </h2>
<p>
  <a href="https://twitter.com/intent/tweet?url=https://akash.mittal.blog/Guardrails-on-AI-Tools-like-ChatGPT-Needed-to-Protect-Secrets.html" target="_blank">
  <i class="fa fa-twitter"></i> Share on Twitter
</a>
</br>
<a href="https://www.linkedin.com/shareArticle?url=https://akash.mittal.blog/Guardrails-on-AI-Tools-like-ChatGPT-Needed-to-Protect-Secrets.html" target="_blank">
  <i class="fa fa-linkedin"></i> Share on LinkedIn
</a>
</p>
</section>
</body>
</html><!-- Reference URLs:
- https://www.scmagazine.com/home/security-news/artificial-intelligence/guardrails-on-ai-tools-like-chatgpt-needed-to-protect-secrets-cisos-say/
- https://gdpr.eu/
Hashtags: 
#AItools #ChatGPT #CISOs #secrets #guardrails #dataclassification #consent #accountability #humansupervision #auditability
Categories: 
AI Security, Data Protection, Machine Learning-->