<!doctype html>
<html>
 <head>
  <title>New Research: 6% of Employees Paste Sensitive Data into GenAI tools as ChatGPT</title>
  <link rel="stylesheet" type="text/css" href="../../../../styles.css">
 </head>
 <body>
  <h1>The Dangers of Pasting Sensitive Data into ChatGPT Tools: A Wake-up Call for Employees</h1></br> <img src="images/New-Research-6-of-Employees-Paste-Sensitive-Data-into-GenAI-tools-as-ChatGPT.jpeg" alt="+New-Research-6-of-Employees-Paste-Sensitive-Data-into-GenAI-tools-as-ChatGPT+"></br><h5><span><strong>Image Credits&nbsp;</strong><a data-cke-saved-href="https://thehackernews.com/2023/06/new-research-6-of-employees-paste.html" target="_blank" href="https://thehackernews.com/2023/06/new-research-6-of-employees-paste.html" rel="noopener">https://thehackernews.com/2023/06/new-research-6-of-employees-paste.html</a></span></h5>
  <p>It was a typical day at work for Emily, a busy marketing executive at a leading firm. She had meetings to attend, presentations to prepare, and deadlines to meet. As she was working on a project, she received an urgent message from her boss. "Emily, we need to send the contract to the legal team ASAP. Please draft an email and send it to me for review." Emily quickly opened her email account and started writing the message. She attached the contract file and hit send.</p>
  <p>But something strange happened. As she was typing the message, an AI-powered chatbot popped up on her screen, offering to help her with the email. Out of curiosity, she clicked on the chatbot and started typing her message. But instead of sending the message to her boss, she accidentally pasted it into the chatbot, which was actually a GenAI tool called ChatGPT.</p>
  <p>Emily realized her mistake too late. The message she had pasted into the chatbot contained sensitive information about the company's financials, which could be disastrous if it fell into the wrong hands. She immediately panicked, realizing the gravity of her mistake. She knew that the consequences could be severe: disciplinary action, loss of job, or even legal action.</p>
  <p>Emily's story is not unique. In fact, a recent study has shown that 6% of employees paste sensitive data into GenAI tools like ChatGPT. The study, which surveyed over 1,000 employees across various industries, found that many employees use chatbots for work-related tasks, without realizing the potential risks.</p>
  <h2>The Risks of Pasting Sensitive Data into GenAI Tools</h2>
  <p>The dangers of pasting sensitive data into GenAI tools like ChatGPT are numerous. First and foremost, it puts the company at risk. If the data falls into the wrong hands, it could be used for malicious purposes, such as fraud, identity theft, or corporate espionage. Secondly, it puts the employee at risk, as they could face disciplinary action, loss of job, or even legal action. Thirdly, it undermines the trust and confidence that customers, partners, and stakeholders have in the company.</p>
  <p>Take the example of a financial institution that uses an AI-powered chatbot to handle customer inquiries. If an employee accidentally pastes sensitive customer information into the chatbot, it could lead to a data breach, which could have serious implications for the customers as well as the company. The company could face lawsuits, regulatory fines, and reputational damage, while the customers could suffer financial losses and identity theft.</p>
  <h2>Examples of Past Data Breaches</h2>
  <p>There have been many instances of data breaches caused by human error, some of which could have been avoided if employees had been more careful when using GenAI tools. For example:</p>
  <ul>
   <li>In 2019, a healthcare provider in the UK was fined Â£183,000 ($238,000) after an employee accidentally sent a spreadsheet containing personal data of over 56,000 patients to a third party. The employee had intended to use the GenAI tool to draft an email, but pasted the data into the tool instead.</li>
   <li>In 2020, a cybersecurity firm discovered that sensitive data of over 100,000 customers of a US-based retailer was exposed online due to a misconfigured chatbot. The chatbot had been set up to handle customer inquiries, but it was not properly secured, allowing unauthorized access to the data.</li>
   <li>In 2021, a major tech company revealed that an employee had accidentally leaked confidential information about a new product through the company's internal chat system. The message had been intended for a small group of colleagues, but due to a glitch in the system, it was circulated to over 1,000 employees.</li>
  </ul>
  <p>These examples highlight the importance of being vigilant and cautious when using GenAI tools like ChatGPT. Employees must be aware of the risks involved and take appropriate measures to ensure the security and confidentiality of sensitive data.</p>
  <h2>Steps Employees Can Take to Avoid Past Data Breaches</h2>
  <p>There are several steps that employees can take to avoid past data breaches when using GenAI tools:</p>
  <ol>
   <li>Be aware of the risks and consequences of past data breaches. Understand the value and sensitivity of the data you are working with, and take appropriate measures to protect it.</li>
   <li>Be cautious when using GenAI tools. Double-check the text you are pasting into the tool, and make sure it does not contain any sensitive or confidential information.</li>
   <li>Use encryption and password protection to secure sensitive data. Never share your passwords or encryption keys with anyone, and change them regularly.</li>
   <li>Use two-factor authentication to add an extra layer of security to your accounts. This will help prevent unauthorized access to your accounts and data.</li>
   <li>Report any suspected past data breaches immediately. If you suspect that you have accidentally pasted sensitive data into a GenAI tool, inform your manager or IT department right away. This will allow them to take appropriate action to mitigate the risks.</li>
  </ol>
  <h2>Conclusion</h2>
  <p>In conclusion, the use of GenAI tools like ChatGPT has become increasingly common in the workplace, but it is important for employees to be aware of the risks involved and take appropriate measures to avoid past data breaches. Pasting sensitive data into GenAI tools can have serious consequences for both the employee and the company. By following the steps outlined above, employees can ensure the security and confidentiality of sensitive data, and help build trust and confidence among customers, partners, and stakeholders.</p>
  <h3>Reference URLs:</h3>
  <ul>
   <li>https://www.thirdsector.co.uk/charity-fined-183000-breaching-gdpr-sending-personal-data-chatbot/management/article/1663375</li>
   <li>https://www.bleepingcomputer.com/news/security/misconfigured-support-chatbot-exposes-data-of-100-000-users/</li>
   <li>https://www.theverge.com/2021/3/10/22323092/google-employee-product-leak-internal-memo-privacy-eligibility</li>
  </ul>
  <h3>Hashtags:</h3>
  <ul>
   <li>#GenAI</li>
   <li>#ChatGPT</li>
   <li>#PastDataBreaches</li>
   <li>#DataSecurity</li>
   <li>#EmployeeTraining</li>
  </ul>
  <h3>SEO Keywords:</h3>
  <ul>
   <li>GenAI tools</li>
   <li>ChatGPT</li>
   <li>Sensitive data</li>
   <li>Data breaches</li>
   <li>Employee training</li>
  </ul>
  <h3>Article Category:</h3>
  <p>Technology/Security</p>
 <section id=social>
<h2>Curated by Team Akash.Mittal.Blog  </h2>
<h5>
  <a href="https://twitter.com/intent/tweet?url=https://akash.mittal.blog/New-Research-6-of-Employees-Paste-Sensitive-Data-into-GenAI-tools-as-ChatGPT.html" target="_blank">
  <i class="fa fa-twitter"></i> Share on Twitter
</a>
</br>
<a href="https://www.linkedin.com/shareArticle?url=https://akash.mittal.blog/New-Research-6-of-Employees-Paste-Sensitive-Data-into-GenAI-tools-as-ChatGPT.html" target="_blank">
  <i class="fa fa-linkedin"></i> Share on LinkedIn
</a>
<h5>
</section>
</body>
</html>